<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
<title>oomph-lib: general_mpi.txt File Reference</title>
<link rel="apple-touch-icon" sizes="57x57" href="../../../figures/apple-touch-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="../../../figures/apple-touch-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="../../../figures/apple-touch-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="../../../figures/apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="../../../figures/apple-touch-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="../../../figures/apple-touch-icon-120x120.png">
<link rel="icon" type="image/png" href="../../../figures/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="../../../figures/favicon-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="../../../figures/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="../../../figures/manifest.json">
<link rel="mask-icon" href="../../../figures/safari-pinned-tab.svg" color="#008000">
<link rel="shortcut icon" href="../../../figures/favicon.ico">
<meta name="msapplication-TileColor" content="#00a300">
<meta name="msapplication-config" content="../../../figures/browserconfig.xml">
<meta name="theme-color" content="#008000">
<link href="http://fonts.googleapis.com/css?family=Open+Sans:400,300,600" rel="stylesheet" type="text/css">
<!-- Doxygen css-->
<!-- <link rel="stylesheet" type="text/css" href="doxygen.css"> -->
<!-- Bootstrap -->
<link href="../../../css/bootstrap.css" rel="stylesheet">
<!-- oomph-lib specific overrides -->
<link rel="stylesheet" type="text/css" href="../../../css/oomph_header.css">
</head>
<body>
<nav class="navbar navbar-default">
<div class="container">
<div class="container-fluid">
  <!-- Brand and toggle get grouped for better mobile display -->
  <div class="navbar-header">
    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="../../../html/index.html"><img alt="oomph-lib" src="../../../figures/oomph_logo.png"></a>
  </div>
  <!-- Collect the nav links, forms, and other content for toggling -->
  <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
    <ul class="nav navbar-nav">          
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Documentation <span class="caret"></span></a>
        <ul class="dropdown-menu">
          <li class="dropdown-header">Big picture</li>
          <li><a href="../../../../doc/intro/html/index.html">The finite element method</a></li>
          <li><a href="../../../../doc/the_data_structure/html/index.html">The data structure</a></li>
          <li><a href="../../../../doc/quick_guide/html/index.html">Not-so-quick guide</a></li>
          <li><a href="../../../../doc/optimisation/html/index.html">Optimisation</a></li>
          <li><a href="../../../../doc/order_of_action_functions/html/index.html">Order of action functions</a></li>
          <li role="separator" class="divider"></li>
          <li class="dropdown-header">Example codes and tutorials</li>
          <li><a href="../../../../doc/example_code_list/html/index.html">List of example codes and tutorials</a></li>
          <li><a href="../../../../doc/example_code_list/html/index.html#meshes">Meshing</a></li>
          <li><a href="../../../../doc/example_code_list/html/index.html#solvers">Solvers</a></li>
          <li><a href="../../../../doc/example_code_list/html/index.html#parallel">MPI parallel processing</a></li>
          <li><a href="../../../../doc/example_code_list/html/index.html#visualisation">Post-processing/visualisation</a></li>
          <li role="separator" class="divider"></li>
          <li class="dropdown-header">Other</li>
          <li><a href="../../../../doc/change_log/html/index.html">Change log</a></li>
          <li><a href="../../../../doc/creating_doc/html/index.html">Creating documentation</a></li>
          <li><a href="../../../../doc/coding_conventions/html/index.html">Coding conventions</a></li>
          <li><a href="../../../../doc/index/html/index.html">Index</a></li>
          <li><a href="../../../../doc/FAQ/html/index.html">FAQ</a></li>
        </ul>
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Get it <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../../../../doc/the_distribution/html/index.html">Installation guide</a></li>
            <li><a href="../../../../doc/subversion/html/index.html">Get code from subversion repository</a></li>
            <li><a href="../../../../doc/download/html/index.html">Get code as tar file</a></li>
            <li><a href="../../../../doc/copyright/html/index.html">Copyright</a></li>
          </ul>
        </li>
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">About <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../../../../doc/people/html/index.html">People</a></li>            
            <li><a href="../../../../doc/contact/html/index.html">Contact/Get involved</a></li>
            <li><a href="../../../../doc/publications/html/index.html">Publications</a></li>
            <li><a href="../../../../doc/acknowledgements/html/index.html">Acknowledgements</a></li>
            <li><a href="../../../../doc/picture_show/index.html">Picture show</a></li>
          </ul>
        </li>
      </li>
    </ul>
    <ul class="nav navbar-nav navbar-right navbar-search">
      <form class="navbar-form" role="search" action="../../../../doc/search_results/html/index.html">
        <div class="input-group">
          <input type="text" class="form-control" placeholder="Search" name="q">
          <span class="input-group-btn">
            <button class="btn btn-default" type="submit">Go</button>
          </span>
        </div><!-- /input-group -->
       <!--<div class="form-group">
          <input type="text" class="form-control" placeholder="Search">
        </div>
        <button type="submit" class="btn btn-default">Submit</button>-->
      </form>
    </ul>
  </div><!-- /.navbar-collapse -->
</div><!-- /.container-fluid -->
</div>
</nav>
<!-- Generated by Doxygen 1.8.17 -->
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle">
<div class="title">general_mpi.txt File Reference</div>  </div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a8b4b24e305d32964339177a53bbbf0d1"><td class="memItemLeft" align="right" valign="top">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt; a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a &gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt; a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a &gt; The only <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward a single call to c <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> majority of c oomph lib s multi physics helper&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a8b4b24e305d32964339177a53bbbf0d1">functions</a> (&lt; em &gt; e.g.&lt;/em &gt; <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> automatic setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> fluid load on solid <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> in FSI <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a>;<a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> determination of &quot;source <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a>&quot; in multi-field <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a>;etc) can be used in distributed problems. For less-straightforward <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a></td></tr>
<tr class="separator:a8b4b24e305d32964339177a53bbbf0d1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a20237347d4fd9890f78a86a6c791c889"><td class="memItemLeft" align="right" valign="top">endcode n Running the code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on the specific problem In most the most computationally expensive tasks are the setup of the Jacobian matrix and the solution of the linear systems When a driver code is run on multiple each processor assembles contributions to the Jacobian matrix from a different subset of the dividing the work of the assembly between <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> In our the assembly of the Jacobian matrix tends to scale very well with the number of <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> The <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> performance of&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> (third-party) linear solvers available from within \c oomph-lib varies greatly and their performance is also strongly dependent on the underlying hardware</td></tr>
<tr class="separator:a20237347d4fd9890f78a86a6c791c889"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a5c60c1f0bdfccd2ab5faedc8135320aa"><td class="memItemLeft" align="right" valign="top">mainpage Parallel processing c oomph lib is designed so&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a></td></tr>
<tr class="separator:a5c60c1f0bdfccd2ab5faedc8135320aa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a32318aade1c612d244cd336e0ceea5b7"><td class="memItemLeft" align="right" valign="top">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a></td></tr>
<tr class="separator:a32318aade1c612d244cd336e0ceea5b7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9b70270f19cce3ae52cb4e5690ce39aa"><td class="memItemLeft" align="right" valign="top">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a9b70270f19cce3ae52cb4e5690ce39aa">basics</a></td></tr>
<tr class="separator:a9b70270f19cce3ae52cb4e5690ce39aa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7899efb3fdcf2096a8ff9a2c6882c752"><td class="memItemLeft" align="right" valign="top">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a7899efb3fdcf2096a8ff9a2c6882c752">iteration</a></td></tr>
<tr class="separator:a7899efb3fdcf2096a8ff9a2c6882c752"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8e5f8f7890b2068b889fad478cd9bf2b"><td class="memItemLeft" align="right" valign="top">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt; a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a &gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt; a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a &gt; The only <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a></td></tr>
<tr class="separator:a8e5f8f7890b2068b889fad478cd9bf2b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a841aef979a2437ad61f7a57455985e99"><td class="memItemLeft" align="right" valign="top">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt; a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a &gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt; a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a &gt; The only <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward a single call to c <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> majority of c oomph lib s multi physics helper <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> user may have to intervene in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution process and or be aware of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> consequences of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a841aef979a2437ad61f7a57455985e99">Hence</a></td></tr>
<tr class="separator:a841aef979a2437ad61f7a57455985e99"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a56da20c03154f15d6bd52e24a26d4d91"><td class="memItemLeft" align="right" valign="top">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt; a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a &gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt; a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a &gt; The only <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward a single call to c <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> majority of c oomph lib s multi physics helper <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> user may have to intervene in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution process and or be aware of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> consequences of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> section ref domain_decomposition provides an overview of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> underlying design used for problem distribution within c oomph lib A number of&lt; a href=&quot;../../../example_code_list/html/index.html#<a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a>&quot;&gt; demo driver codes for distributed <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a>&lt;/a &gt; are provided and any additional issues are discussed in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> accompanying tutorials n n&lt; HR &gt;&lt; HR &gt; section <a class="el" href="general__mpi_8txt.html#a9b70270f19cce3ae52cb4e5690ce39aa">basics</a> Basic <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> usage subsection installation How to build install oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> To compile c oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> you must specify <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> configure flag n n code enable MPI endcode n If you use c oomph lib s&lt; code &gt; autogen sh&lt;/code &gt; script to build <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library you should add this line to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a>&lt; code &gt; config configure_options current&lt;/code &gt; file You should also ensure <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> appropriate <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> compilers are specified by <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> c&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a56da20c03154f15d6bd52e24a26d4d91">CXX</a></td></tr>
<tr class="separator:a56da20c03154f15d6bd52e24a26d4d91"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2ddf49a0be288edadd9ec711f1fad73a"><td class="memItemLeft" align="right" valign="top">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt; a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a &gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt; a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a &gt; The only <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward a single call to c <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> majority of c oomph lib s multi physics helper <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> user may have to intervene in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution process and or be aware of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> consequences of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> section ref domain_decomposition provides an overview of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> underlying design used for problem distribution within c oomph lib A number of&lt; a href=&quot;../../../example_code_list/html/index.html#<a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a>&quot;&gt; demo driver codes for distributed <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a>&lt;/a &gt; are provided and any additional issues are discussed in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> accompanying tutorials n n&lt; HR &gt;&lt; HR &gt; section <a class="el" href="general__mpi_8txt.html#a9b70270f19cce3ae52cb4e5690ce39aa">basics</a> Basic <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> usage subsection installation How to build install oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> To compile c oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> you must specify <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> configure flag n n code enable MPI endcode n If you use c oomph lib s&lt; code &gt; autogen sh&lt;/code &gt; script to build <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library you should add this line to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a>&lt; code &gt; config configure_options current&lt;/code &gt; file You should also ensure <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> appropriate <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> compilers are specified by <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> c c&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a2ddf49a0be288edadd9ec711f1fad73a">CC</a></td></tr>
<tr class="separator:a2ddf49a0be288edadd9ec711f1fad73a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4547f3139c2429c2424a100dd9e464e8"><td class="memItemLeft" align="right" valign="top">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt; a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a &gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt; a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a &gt; The only <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward a single call to c <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> majority of c oomph lib s multi physics helper <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> user may have to intervene in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution process and or be aware of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> consequences of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> section ref domain_decomposition provides an overview of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> underlying design used for problem distribution within c oomph lib A number of&lt; a href=&quot;../../../example_code_list/html/index.html#<a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a>&quot;&gt; demo driver codes for distributed <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a>&lt;/a &gt; are provided and any additional issues are discussed in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> accompanying tutorials n n&lt; HR &gt;&lt; HR &gt; section <a class="el" href="general__mpi_8txt.html#a9b70270f19cce3ae52cb4e5690ce39aa">basics</a> Basic <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> usage subsection installation How to build install oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> To compile c oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> you must specify <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> configure flag n n code enable MPI endcode n If you use c oomph lib s&lt; code &gt; autogen sh&lt;/code &gt; script to build <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library you should add this line to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a>&lt; code &gt; config configure_options current&lt;/code &gt; file You should also ensure <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> appropriate <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> compilers are specified by <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> c c c F77 and c LD flags For&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a4547f3139c2429c2424a100dd9e464e8">instance</a></td></tr>
<tr class="separator:a4547f3139c2429c2424a100dd9e464e8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa0367dea3ef7bca821f17c6d18c00b44"><td class="memItemLeft" align="right" valign="top">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in parallel Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt; a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a &gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt; a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a &gt; The only parallel task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward a single call to c <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> majority of c oomph lib s multi physics helper <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> user may have to intervene in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution process and or be aware of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> consequences of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> section ref domain_decomposition provides an overview of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> underlying design used for problem distribution within c oomph lib A number of&lt; a href=&quot;../../../example_code_list/html/index.html#parallel&quot;&gt; demo driver codes for distributed <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a>&lt;/a &gt; are provided and any additional issues are discussed in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> accompanying tutorials n n&lt; HR &gt;&lt; HR &gt; section <a class="el" href="general__mpi_8txt.html#a9b70270f19cce3ae52cb4e5690ce39aa">basics</a> Basic parallel usage subsection installation How to build install oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> To compile c oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> you must specify <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> configure flag n n code enable MPI endcode n If you use c oomph lib s&lt; code &gt; autogen sh&lt;/code &gt; script to build <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library you should add this line to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a>&lt; code &gt; config configure_options current&lt;/code &gt; file You should also ensure <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> appropriate parallel compilers are specified by <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> c c c F77 and c LD flags For if you use&lt; a href=&quot;http: you should use \c <a class="el" href="general__mpi_8txt.html#a56da20c03154f15d6bd52e24a26d4d91">CXX</a>=mpic++, \c <a class="el" href="general__mpi_8txt.html#a2ddf49a0be288edadd9ec711f1fad73a">CC</a>=mpicc, \c F77=mpif77 and \c LD=mpif77. \n\n- When \c oomph-lib is built with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a>, <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> macro \c OOMPH_HAS_MPI is defined. It is used to isolate parallel sections of code to ensure <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library can be used in serial and parallel: &lt;em&gt; e.g. &lt;/em&gt; \n\n \code [...] #ifdef OOMPH_HAS_MPI std::cout &lt;&lt; &quot;This code has been compiled with mpi <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> \n &quot; &lt;&lt; &quot;and is running on &quot; &lt;&lt; Communicator_pt-&gt; nproc () &lt;&lt; &quot; processors. &quot; &lt;&lt; std std::cout&lt;&lt; &quot;This code has been compiled without mpi <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a>&quot; &lt;&lt; std::endl; #endif [...] \endcode \n.########################################################################&lt;HR&gt; subsection running How to run a driver code in parallel MPI&lt;strong&gt; must&lt;/strong&gt; be initialised in every driver code <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> is to be run in&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a></td></tr>
<tr class="separator:aa0367dea3ef7bca821f17c6d18c00b44"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af907a2a24a4ba5cbc2a07b1b01229f35"><td class="memItemLeft" align="right" valign="top">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#af907a2a24a4ba5cbc2a07b1b01229f35">speedup</a></td></tr>
<tr class="separator:af907a2a24a4ba5cbc2a07b1b01229f35"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a27976d67be78d15a2ae29ab248ac1b68"><td class="memItemLeft" align="right" valign="top">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a27976d67be78d15a2ae29ab248ac1b68">applications</a></td></tr>
<tr class="separator:a27976d67be78d15a2ae29ab248ac1b68"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a26148a209e819999805bbbeaec42960d"><td class="memItemLeft" align="right" valign="top">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple processors should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive tasks are <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems When a driver code is run on multiple&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a></td></tr>
<tr class="separator:a26148a209e819999805bbbeaec42960d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4fea119ad352a5e9af4b03c3e6fbb6da"><td class="memItemLeft" align="right" valign="top">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive tasks are <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems When a driver code is run on multiple each processor assembles contributions to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix from a different subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a></td></tr>
<tr class="separator:a4fea119ad352a5e9af4b03c3e6fbb6da"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a07c5407165a6de8c6d14fa82165c75d8"><td class="memItemLeft" align="right" valign="top">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive tasks are <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems When a driver code is run on multiple each processor assembles contributions to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix from a different subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> dividing <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> work of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly between <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> In our&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a07c5407165a6de8c6d14fa82165c75d8">experience</a></td></tr>
<tr class="separator:a07c5407165a6de8c6d14fa82165c75d8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae749d01bee77d0ded7e8930a7957cd4b"><td class="memItemLeft" align="right" valign="top">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive tasks are <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems When a driver code is run on multiple each processor assembles contributions to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix from a different subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> dividing <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> work of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly between <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> In our <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix tends to scale very well with <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> number of <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> The <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> performance of&lt; em &gt; e g&lt;/em &gt; <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> speed of your machine s&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#ae749d01bee77d0ded7e8930a7957cd4b">interconnects</a></td></tr>
<tr class="separator:ae749d01bee77d0ded7e8930a7957cd4b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a58dff0adb83d6245f6d35c3df6615412"><td class="memItemLeft" align="right" valign="top">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive tasks are <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems When a driver code is run on multiple each processor assembles contributions to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix from a different subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> dividing <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> work of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly between <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> In our <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix tends to scale very well with <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> number of <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> The <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> performance of&lt; em &gt; e g&lt;/em &gt; <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> speed of your machine s etc n n The MPI header file c mpi h is included in c oomph lib s generic&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a58dff0adb83d6245f6d35c3df6615412">header</a></td></tr>
<tr class="separator:a58dff0adb83d6245f6d35c3df6615412"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a87cbf19b7098172f49ad631a0f249d"><td class="memItemLeft" align="right" valign="top">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive tasks are <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems When a driver code is run on multiple each processor assembles contributions to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix from a different subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> dividing <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> work of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly between <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> In our <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix tends to scale very well with <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> number of <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> The <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> performance of&lt; em &gt; e g&lt;/em &gt; <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> speed of your machine s etc n n The MPI <a class="el" href="general__mpi_8txt.html#a58dff0adb83d6245f6d35c3df6615412">header</a> file c mpi h is included in c oomph lib s generic so it is not necessary to include it in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> driver code The <a class="el" href="general__mpi_8txt.html#a8b4b24e305d32964339177a53bbbf0d1">functions</a> c MPI_Helpers::init(...) and \c MPI_Helpers see&lt; a href=&quot;../../../linear_solvers/html/index.html&quot;&gt; the linear solver tutorial&lt;/a &gt; for details n n c oomph lib s block preconditioning framework is fully parallelised and can be used in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> same way as in a serial code&lt; HR &gt; subsection self_tests How to include <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> demo codes into <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> self tests The configure flag c with mpi self tests includes c oomph lib s <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> demo driver codes into <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> self tests executed when c make c check is run The self tests require <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> executable to be run on two <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> command <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> spawns a two processor <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> job on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> target machine must be specified as an argument to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> configure flag For&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="general__mpi_8txt.html#a1a87cbf19b7098172f49ad631a0f249d">example</a></td></tr>
<tr class="separator:a1a87cbf19b7098172f49ad631a0f249d"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Function Documentation</h2>
<a id="a8b4b24e305d32964339177a53bbbf0d1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8b4b24e305d32964339177a53bbbf0d1">&#9670;&nbsp;</a></span>functions()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt;a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a&gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt;a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a&gt; The only <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward a single call to c <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> majority of c oomph lib s multi physics helper functions </td>
          <td>(</td>
          <td class="paramtype">&lt; em &gt; e.g.&lt;/em &gt; <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> automatic setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> fluid load on solid <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> in FSI <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a>;<a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> determination of &quot;source <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a>&quot; in multi-field <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a>;&#160;</td>
          <td class="paramname"><em>etc</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a20237347d4fd9890f78a86a6c791c889"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a20237347d4fd9890f78a86a6c791c889">&#9670;&nbsp;</a></span>the()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">endcode n Running the code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on the specific problem In most the most computationally expensive tasks are the setup of the Jacobian matrix and the solution of the linear systems When a driver code is run on multiple each processor assembles contributions to the Jacobian matrix from a different subset of the dividing the work of the assembly between <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> In our the assembly of the Jacobian matrix tends to scale very well with the number of <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> The <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> performance of the </td>
          <td>(</td>
          <td class="paramtype">third-&#160;</td>
          <td class="paramname"><em>party</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a27976d67be78d15a2ae29ab248ac1b68"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a27976d67be78d15a2ae29ab248ac1b68">&#9670;&nbsp;</a></span>applications</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most applications</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00122">122</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a9b70270f19cce3ae52cb4e5690ce39aa"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9b70270f19cce3ae52cb4e5690ce39aa">&#9670;&nbsp;</a></span>basics</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref basics</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00005">5</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a2ddf49a0be288edadd9ec711f1fad73a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2ddf49a0be288edadd9ec711f1fad73a">&#9670;&nbsp;</a></span>CC</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt;a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a&gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt;a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a&gt; The only <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward a single call to c <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> majority of c oomph lib s multi physics helper <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> user may have to intervene in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution process and or be aware of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> consequences of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> section ref domain_decomposition provides an overview of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> underlying design used for problem distribution within c oomph lib A number of&lt;a href=&quot;../../../example_code_list/html/index.html#<a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a>&quot;&gt; demo driver codes for distributed <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a>&lt;/a&gt; are provided and any additional issues are discussed in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> accompanying tutorials n n&lt;HR&gt;&lt;HR&gt; section <a class="el" href="general__mpi_8txt.html#a9b70270f19cce3ae52cb4e5690ce39aa">basics</a> Basic <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> usage subsection installation How to build install oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> To compile c oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> you must specify <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> configure flag n n code enable MPI endcode n If you use c oomph lib s&lt;code&gt; autogen sh&lt;/code&gt; script to build <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library you should add this line to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a>&lt;code&gt; config configure_options current&lt;/code&gt; file You should also ensure <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> appropriate <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> compilers are specified by <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> c c CC</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00059">59</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a56da20c03154f15d6bd52e24a26d4d91"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a56da20c03154f15d6bd52e24a26d4d91">&#9670;&nbsp;</a></span>CXX</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt;a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a&gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt;a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a&gt; The only <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward a single call to c <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> majority of c oomph lib s multi physics helper <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> user may have to intervene in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution process and or be aware of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> consequences of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> section ref domain_decomposition provides an overview of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> underlying design used for problem distribution within c oomph lib A number of&lt;a href=&quot;../../../example_code_list/html/index.html#<a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a>&quot;&gt; demo driver codes for distributed <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a>&lt;/a&gt; are provided and any additional issues are discussed in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> accompanying tutorials n n&lt;HR&gt;&lt;HR&gt; section <a class="el" href="general__mpi_8txt.html#a9b70270f19cce3ae52cb4e5690ce39aa">basics</a> Basic <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> usage subsection installation How to build install oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> To compile c oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> you must specify <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> configure flag n n code enable MPI endcode n If you use c oomph lib s&lt;code&gt; autogen sh&lt;/code&gt; script to build <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library you should add this line to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a>&lt;code&gt; config configure_options current&lt;/code&gt; file You should also ensure <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> appropriate <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> compilers are specified by <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> c CXX</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00059">59</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a4fea119ad352a5e9af4b03c3e6fbb6da"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4fea119ad352a5e9af4b03c3e6fbb6da">&#9670;&nbsp;</a></span>elements</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive tasks are <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems When a driver code is run on multiple each processor assembles contributions to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix from a different subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> elements</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00126">126</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a1a87cbf19b7098172f49ad631a0f249d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1a87cbf19b7098172f49ad631a0f249d">&#9670;&nbsp;</a></span>example</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive tasks are <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems When a driver code is run on multiple each processor assembles contributions to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix from a different subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> dividing <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> work of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly between <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> In our <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix tends to scale very well with <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> number of <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> The <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> performance of&lt;em&gt; e g&lt;/em&gt; <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> speed of your machine s etc n n The MPI <a class="el" href="general__mpi_8txt.html#a58dff0adb83d6245f6d35c3df6615412">header</a> file c mpi h is included in c oomph lib s generic so it is not necessary to include it in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> driver code The <a class="el" href="general__mpi_8txt.html#a8b4b24e305d32964339177a53bbbf0d1">functions</a> c MPI_Helpers::init (...) and \c MPI_Helpers see&lt;a href=&quot;../../../linear_solvers/html/index.html&quot;&gt; the linear solver tutorial&lt;/a&gt; for details n n c oomph lib s block preconditioning framework is fully parallelised and can be used in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> same way as in a serial code&lt;HR&gt; subsection self_tests How to include <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> demo codes into <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> self tests The configure flag c with mpi self tests includes c oomph lib s <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> demo driver codes into <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> self tests executed when c make c check is run The self tests require <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> executable to be run on two <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> command <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> spawns a two processor <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> job on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> target machine must be specified as an argument to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> configure flag For example</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00138">138</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a07c5407165a6de8c6d14fa82165c75d8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a07c5407165a6de8c6d14fa82165c75d8">&#9670;&nbsp;</a></span>experience</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive tasks are <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems When a driver code is run on multiple each processor assembles contributions to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix from a different subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> dividing <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> work of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly between <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> In our experience</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00127">127</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a58dff0adb83d6245f6d35c3df6615412"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a58dff0adb83d6245f6d35c3df6615412">&#9670;&nbsp;</a></span>header</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive tasks are <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems When a driver code is run on multiple each processor assembles contributions to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix from a different subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> dividing <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> work of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly between <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> In our <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix tends to scale very well with <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> number of <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> The <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> performance of&lt;em&gt; e g&lt;/em&gt; <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> speed of your machine s etc n n The MPI header file c mpi h is included in c oomph lib s generic header</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00137">137</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a841aef979a2437ad61f7a57455985e99"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a841aef979a2437ad61f7a57455985e99">&#9670;&nbsp;</a></span>Hence</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt;a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a&gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt;a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a&gt; The only <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward a single call to c <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> majority of c oomph lib s multi physics helper <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> user may have to intervene in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution process and or be aware of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> consequences of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution Hence</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00032">32</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a4547f3139c2429c2424a100dd9e464e8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4547f3139c2429c2424a100dd9e464e8">&#9670;&nbsp;</a></span>instance</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt;a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a&gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt;a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a&gt; The only <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward a single call to c <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> majority of c oomph lib s multi physics helper <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> user may have to intervene in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution process and or be aware of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> consequences of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> section ref domain_decomposition provides an overview of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> underlying design used for problem distribution within c oomph lib A number of&lt;a href=&quot;../../../example_code_list/html/index.html#<a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a>&quot;&gt; demo driver codes for distributed <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a>&lt;/a&gt; are provided and any additional issues are discussed in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> accompanying tutorials n n&lt;HR&gt;&lt;HR&gt; section <a class="el" href="general__mpi_8txt.html#a9b70270f19cce3ae52cb4e5690ce39aa">basics</a> Basic <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> usage subsection installation How to build install oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> To compile c oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> you must specify <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> configure flag n n code enable MPI endcode n If you use c oomph lib s&lt;code&gt; autogen sh&lt;/code&gt; script to build <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library you should add this line to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a>&lt;code&gt; config configure_options current&lt;/code&gt; file You should also ensure <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> appropriate <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> compilers are specified by <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> c c c F77 and c LD flags For instance</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00059">59</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="ae749d01bee77d0ded7e8930a7957cd4b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae749d01bee77d0ded7e8930a7957cd4b">&#9670;&nbsp;</a></span>interconnects</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive tasks are <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems When a driver code is run on multiple each processor assembles contributions to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix from a different subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> dividing <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> work of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly between <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> In our <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix tends to scale very well with <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> number of <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> The <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> performance of&lt;em&gt; e g&lt;/em&gt; <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> speed of your machine s interconnects</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00134">134</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a7899efb3fdcf2096a8ff9a2c6882c752"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7899efb3fdcf2096a8ff9a2c6882c752">&#9670;&nbsp;</a></span>iteration</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton iteration</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00012">12</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="aa0367dea3ef7bca821f17c6d18c00b44"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa0367dea3ef7bca821f17c6d18c00b44">&#9670;&nbsp;</a></span>parallel</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in parallel Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt;a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a&gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt;a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a&gt; The only parallel task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward a single call to c <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> majority of c oomph lib s multi physics helper <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> user may have to intervene in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution process and or be aware of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> consequences of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> section ref domain_decomposition provides an overview of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> underlying design used for problem distribution within c oomph lib A number of&lt;a href=&quot;../../../example_code_list/html/index.html#parallel&quot;&gt; demo driver codes for distributed <a class="el" href="general__mpi_8txt.html#a8e5f8f7890b2068b889fad478cd9bf2b">problems</a>&lt;/a&gt; are provided and any additional issues are discussed in <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> accompanying tutorials n n&lt;HR&gt;&lt;HR&gt; section <a class="el" href="general__mpi_8txt.html#a9b70270f19cce3ae52cb4e5690ce39aa">basics</a> Basic parallel usage subsection installation How to build install oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> To compile c oomph lib with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> you must specify <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> configure flag n n code enable MPI endcode n If you use c oomph lib s&lt;code&gt; autogen sh&lt;/code&gt; script to build <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library you should add this line to <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a>&lt;code&gt; config configure_options current&lt;/code&gt; file You should also ensure <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> appropriate parallel compilers are specified by <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> c c c F77 and c LD flags For if you use&lt;a href=&quot;http: you should use \c <a class="el" href="general__mpi_8txt.html#a56da20c03154f15d6bd52e24a26d4d91">CXX</a>=mpic++, \c <a class="el" href="general__mpi_8txt.html#a2ddf49a0be288edadd9ec711f1fad73a">CC</a>=mpicc, \c F77=mpif77 and \c LD=mpif77. \n\n- When \c oomph-lib is built with MPI <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a>, <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> macro \c OOMPH_HAS_MPI is defined. It is used to isolate parallel sections of code to ensure <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library can be used in serial and parallel: &lt;em&gt; e.g. &lt;/em&gt; \n\n \code [...] #ifdef OOMPH_HAS_MPI std::cout &lt;&lt; &quot;This code has been compiled with mpi <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a> \n &quot; &lt;&lt; &quot;and is running on &quot; &lt;&lt; Communicator_pt-&gt; nproc () &lt;&lt; &quot; processors. &quot; &lt;&lt; std std::cout&lt;&lt; &quot;This code has been compiled without mpi <a class="el" href="general__mpi_8txt.html#a32318aade1c612d244cd336e0ceea5b7">support</a>&quot; &lt;&lt; std::endl; #endif [...] \endcode \n.########################################################################&lt;HR&gt; subsection running How to run a driver code in parallel MPI&lt;strong&gt; must&lt;/strong&gt; be initialised in every driver code <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> is to be run in parallel</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00075">75</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a8e5f8f7890b2068b889fad478cd9bf2b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8e5f8f7890b2068b889fad478cd9bf2b">&#9670;&nbsp;</a></span>problems</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI discussed below in ref many of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive phases of a typical computation are automatically performed in <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> Examples of automatically parallelised tasks include The assembly of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> residual vector in Newton s method Error estimation The solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems within <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Newton and any preconditioning operations performed within c oomph lib s&lt;a href=&quot;../../../mpi/block_preconditioners/html/index.html&quot;&gt; block preconditioning framework&lt;/a&gt; which relies heavily on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library s&lt;a href=&quot;../../../mpi/distributed_linear_algebra_infrastructure/html/index.html&quot;&gt; distributed linear algebra infrastructure&lt;/a&gt; The only <a class="el" href="general__mpi_8txt.html#aa0367dea3ef7bca821f17c6d18c00b44">parallel</a> task <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> requires user intervention is <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> distribution of a problem over multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> so <a class="el" href="general__mpi_8txt.html#a5c60c1f0bdfccd2ab5faedc8135320aa">that</a> each processor stores a subset of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> <a class="el" href="general__mpi_8txt.html#a4fea119ad352a5e9af4b03c3e6fbb6da">elements</a> For straightforward problems</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00024">24</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a26148a209e819999805bbbeaec42960d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a26148a209e819999805bbbeaec42960d">&#9670;&nbsp;</a></span>processors</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple processors should immediately lead to a although this depends on <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> specific problem In most <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> most computationally expensive tasks are <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> setup of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> Jacobian matrix and <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> solution of <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> linear systems When a driver code is run on multiple processors</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00124">124</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="af907a2a24a4ba5cbc2a07b1b01229f35"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af907a2a24a4ba5cbc2a07b1b01229f35">&#9670;&nbsp;</a></span>speedup</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">endcode n Running <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> code on multiple <a class="el" href="general__mpi_8txt.html#a26148a209e819999805bbbeaec42960d">processors</a> should immediately lead to a speedup</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00121">121</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a32318aade1c612d244cd336e0ceea5b7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a32318aade1c612d244cd336e0ceea5b7">&#9670;&nbsp;</a></span>support</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">mainpage Parallel processing c oomph lib is designed so provided <a class="el" href="general__mpi_8txt.html#a20237347d4fd9890f78a86a6c791c889">the</a> library is compiled with MPI support</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00004">4</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
<a id="a5c60c1f0bdfccd2ab5faedc8135320aa"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5c60c1f0bdfccd2ab5faedc8135320aa">&#9670;&nbsp;</a></span>that</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">mainpage Parallel processing c oomph lib is designed so that</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="general__mpi_8txt_source.html#l00003">3</a> of file <a class="el" href="general__mpi_8txt_source.html">general_mpi.txt</a>.</p>

</div>
</div>
</div><!-- contents -->

    <!-- jQuery for Bootstrap and Doxygen -->
    <script src="../../../js/jquery-1.12.0.min.js"></script>
    <!-- Minified boostrap plugins-->
    <script src="../../../js/bootstrap.js"></script>
    <!-- Doxygen dependency to add powertips to source code-->
    <script src="../../../js/jquery.powertip.min.js"></script>
    <!-- The  following script is generated by doxygen and hides/shows levels in 
         the data structure lists and adds powertips to source code-->
    <script src="../../../js/dynsections.js" ></script>
    <!-- add to Doxygen's class names so bootstrap css and js recognises them-->
    <script type="text/javascript">
    $(".contents").addClass("container");
    $(".header").addClass("container");
    $(".navpath").addClass("container");
    $("#navrow3").addClass("container");
    $("#navrow4").addClass("container");
    $(".mlabel").addClass("label");
    $(".mlabel").addClass("label-default");
    $(".memitem").addClass("panel");
    $(".memitem").addClass("panel-info");
    $(".memproto").addClass("panel-heading");
    $(".memdoc").addClass("panel-body");
    </script>
    <footer>
      <div class="container">
        <div class="text-muted" style="float:right;">Generated by <a href="http://www.doxygen.org/index.html">
          <img style="height:18px;" class="footer-img" src="doxygen.png" alt="doxygen"></a> on Fri Jul 9 2021 21:20:41
        </div>
      </div>
    </footer>
</body>
</html>
