\hypertarget{general__mpi_8txt}{}\doxysection{general\+\_\+mpi.\+txt File Reference}
\label{general__mpi_8txt}\index{general\_mpi.txt@{general\_mpi.txt}}
\doxysubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$ a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a $>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$ a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a $>$ The only \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward a single call to c \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} majority of c oomph lib s multi physics helper \mbox{\hyperlink{general__mpi_8txt_a8b4b24e305d32964339177a53bbbf0d1}{functions}} ($<$ em $>$ e.\+g.$<$/em $>$ \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} automatic setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} fluid load on solid \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} in F\+SI \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}};\mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} determination of \char`\"{}source \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}}\char`\"{} in multi-\/field \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}};etc) can be used in distributed problems. For less-\/straightforward \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}}
\item 
endcode n Running the code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on the specific problem In most the most computationally expensive tasks are the setup of the Jacobian matrix and the solution of the linear systems When a driver code is run on multiple each processor assembles contributions to the Jacobian matrix from a different subset of the dividing the work of the assembly between \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} In our the assembly of the Jacobian matrix tends to scale very well with the number of \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} The \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} performance of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} (third-\/party) linear solvers available from within \textbackslash{}c oomph-\/lib varies greatly and their performance is also strongly dependent on the underlying hardware
\end{DoxyCompactItemize}
\doxysubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
mainpage Parallel processing c oomph lib is designed so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}}
\item 
mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}}
\item 
mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref \mbox{\hyperlink{general__mpi_8txt_a9b70270f19cce3ae52cb4e5690ce39aa}{basics}}
\item 
mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton \mbox{\hyperlink{general__mpi_8txt_a7899efb3fdcf2096a8ff9a2c6882c752}{iteration}}
\item 
mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$ a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a $>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$ a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a $>$ The only \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}}
\item 
mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$ a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a $>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$ a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a $>$ The only \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward a single call to c \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} majority of c oomph lib s multi physics helper \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} user may have to intervene in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution process and or be aware of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} consequences of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution \mbox{\hyperlink{general__mpi_8txt_a841aef979a2437ad61f7a57455985e99}{Hence}}
\item 
mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$ a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a $>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$ a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a $>$ The only \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward a single call to c \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} majority of c oomph lib s multi physics helper \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} user may have to intervene in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution process and or be aware of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} consequences of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} section ref domain\+\_\+decomposition provides an overview of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} underlying design used for problem distribution within c oomph lib A number of$<$ a href=\char`\"{}../../../example\+\_\+code\+\_\+list/html/index.\+html\#\mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}}\char`\"{}$>$ demo driver codes for distributed \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}}$<$/a $>$ are provided and any additional issues are discussed in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} accompanying tutorials n n$<$ HR $>$$<$ HR $>$ section \mbox{\hyperlink{general__mpi_8txt_a9b70270f19cce3ae52cb4e5690ce39aa}{basics}} Basic \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} usage subsection installation How to build install oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} To compile c oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} you must specify \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} configure flag n n code enable M\+PI endcode n If you use c oomph lib s$<$ code $>$ autogen sh$<$/code $>$ script to build \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library you should add this line to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}}$<$ code $>$ config configure\+\_\+options current$<$/code $>$ file You should also ensure \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} appropriate \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} compilers are specified by \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} c \mbox{\hyperlink{general__mpi_8txt_a56da20c03154f15d6bd52e24a26d4d91}{C\+XX}}
\item 
mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$ a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a $>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$ a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a $>$ The only \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward a single call to c \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} majority of c oomph lib s multi physics helper \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} user may have to intervene in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution process and or be aware of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} consequences of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} section ref domain\+\_\+decomposition provides an overview of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} underlying design used for problem distribution within c oomph lib A number of$<$ a href=\char`\"{}../../../example\+\_\+code\+\_\+list/html/index.\+html\#\mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}}\char`\"{}$>$ demo driver codes for distributed \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}}$<$/a $>$ are provided and any additional issues are discussed in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} accompanying tutorials n n$<$ HR $>$$<$ HR $>$ section \mbox{\hyperlink{general__mpi_8txt_a9b70270f19cce3ae52cb4e5690ce39aa}{basics}} Basic \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} usage subsection installation How to build install oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} To compile c oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} you must specify \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} configure flag n n code enable M\+PI endcode n If you use c oomph lib s$<$ code $>$ autogen sh$<$/code $>$ script to build \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library you should add this line to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}}$<$ code $>$ config configure\+\_\+options current$<$/code $>$ file You should also ensure \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} appropriate \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} compilers are specified by \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} c c \mbox{\hyperlink{general__mpi_8txt_a2ddf49a0be288edadd9ec711f1fad73a}{CC}}
\item 
mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$ a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a $>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$ a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a $>$ The only \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward a single call to c \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} majority of c oomph lib s multi physics helper \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} user may have to intervene in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution process and or be aware of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} consequences of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} section ref domain\+\_\+decomposition provides an overview of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} underlying design used for problem distribution within c oomph lib A number of$<$ a href=\char`\"{}../../../example\+\_\+code\+\_\+list/html/index.\+html\#\mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}}\char`\"{}$>$ demo driver codes for distributed \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}}$<$/a $>$ are provided and any additional issues are discussed in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} accompanying tutorials n n$<$ HR $>$$<$ HR $>$ section \mbox{\hyperlink{general__mpi_8txt_a9b70270f19cce3ae52cb4e5690ce39aa}{basics}} Basic \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} usage subsection installation How to build install oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} To compile c oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} you must specify \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} configure flag n n code enable M\+PI endcode n If you use c oomph lib s$<$ code $>$ autogen sh$<$/code $>$ script to build \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library you should add this line to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}}$<$ code $>$ config configure\+\_\+options current$<$/code $>$ file You should also ensure \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} appropriate \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} compilers are specified by \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} c c c F77 and c LD flags For \mbox{\hyperlink{general__mpi_8txt_a4547f3139c2429c2424a100dd9e464e8}{instance}}
\item 
mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in parallel Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$ a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a $>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$ a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a $>$ The only parallel task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward a single call to c \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} majority of c oomph lib s multi physics helper \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} user may have to intervene in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution process and or be aware of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} consequences of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} section ref domain\+\_\+decomposition provides an overview of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} underlying design used for problem distribution within c oomph lib A number of$<$ a href=\char`\"{}../../../example\+\_\+code\+\_\+list/html/index.\+html\#parallel\char`\"{}$>$ demo driver codes for distributed \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}}$<$/a $>$ are provided and any additional issues are discussed in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} accompanying tutorials n n$<$ HR $>$$<$ HR $>$ section \mbox{\hyperlink{general__mpi_8txt_a9b70270f19cce3ae52cb4e5690ce39aa}{basics}} Basic parallel usage subsection installation How to build install oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} To compile c oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} you must specify \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} configure flag n n code enable M\+PI endcode n If you use c oomph lib s$<$ code $>$ autogen sh$<$/code $>$ script to build \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library you should add this line to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}}$<$ code $>$ config configure\+\_\+options current$<$/code $>$ file You should also ensure \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} appropriate parallel compilers are specified by \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} c c c F77 and c LD flags For if you use$<$ a href=\char`\"{}http\+: you should use \textbackslash{}c \mbox{\hyperlink{general__mpi_8txt_a56da20c03154f15d6bd52e24a26d4d91}{C\+XX}}=mpic++, \textbackslash{}c \mbox{\hyperlink{general__mpi_8txt_a2ddf49a0be288edadd9ec711f1fad73a}{CC}}=mpicc, \textbackslash{}c F77=mpif77 and \textbackslash{}c LD=mpif77. \textbackslash{}n\textbackslash{}n-\/ When \textbackslash{}c oomph-\/lib is built with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}}, \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} macro \textbackslash{}c O\+O\+M\+P\+H\+\_\+\+H\+A\+S\+\_\+\+M\+PI is defined. It is used to isolate parallel sections of code to ensure \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library can be used in serial and parallel\+: $<$em$>$ e.\+g. $<$/em$>$ \textbackslash{}n\textbackslash{}n \textbackslash{}code \mbox{[}...\mbox{]} \#ifdef O\+O\+M\+P\+H\+\_\+\+H\+A\+S\+\_\+\+M\+PI std\+::cout $<$$<$ \char`\"{}This code has been compiled with mpi \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} \textbackslash{}n \char`\"{} $<$$<$ \char`\"{}and is running on \char`\"{} $<$$<$ Communicator\+\_\+pt-\/$>$ nproc () $<$$<$ \char`\"{} processors. \char`\"{} $<$$<$ std std\+::cout$<$$<$ \char`\"{}This code has been compiled without mpi \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}}\char`\"{} $<$$<$ std\+::endl; \#endif \mbox{[}...\mbox{]} \textbackslash{}endcode \textbackslash{}n.\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#$<$HR$>$ subsection running How to run a driver code in parallel M\+PI$<$strong$>$ must$<$/strong$>$ be initialised in every driver code \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} is to be run in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}}
\item 
endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a \mbox{\hyperlink{general__mpi_8txt_af907a2a24a4ba5cbc2a07b1b01229f35}{speedup}}
\item 
endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a27976d67be78d15a2ae29ab248ac1b68}{applications}}
\item 
endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple processors should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive tasks are \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems When a driver code is run on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}}
\item 
endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive tasks are \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems When a driver code is run on multiple each processor assembles contributions to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix from a different subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}}
\item 
endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive tasks are \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems When a driver code is run on multiple each processor assembles contributions to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix from a different subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} dividing \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} work of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly between \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} In our \mbox{\hyperlink{general__mpi_8txt_a07c5407165a6de8c6d14fa82165c75d8}{experience}}
\item 
endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive tasks are \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems When a driver code is run on multiple each processor assembles contributions to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix from a different subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} dividing \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} work of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly between \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} In our \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix tends to scale very well with \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} number of \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} The \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} performance of$<$ em $>$ e g$<$/em $>$ \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} speed of your machine s \mbox{\hyperlink{general__mpi_8txt_ae749d01bee77d0ded7e8930a7957cd4b}{interconnects}}
\item 
endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive tasks are \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems When a driver code is run on multiple each processor assembles contributions to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix from a different subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} dividing \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} work of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly between \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} In our \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix tends to scale very well with \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} number of \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} The \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} performance of$<$ em $>$ e g$<$/em $>$ \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} speed of your machine s etc n n The M\+PI header file c mpi h is included in c oomph lib s generic \mbox{\hyperlink{general__mpi_8txt_a58dff0adb83d6245f6d35c3df6615412}{header}}
\item 
endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive tasks are \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems When a driver code is run on multiple each processor assembles contributions to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix from a different subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} dividing \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} work of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly between \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} In our \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix tends to scale very well with \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} number of \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} The \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} performance of$<$ em $>$ e g$<$/em $>$ \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} speed of your machine s etc n n The M\+PI \mbox{\hyperlink{general__mpi_8txt_a58dff0adb83d6245f6d35c3df6615412}{header}} file c mpi h is included in c oomph lib s generic so it is not necessary to include it in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} driver code The \mbox{\hyperlink{general__mpi_8txt_a8b4b24e305d32964339177a53bbbf0d1}{functions}} c M\+P\+I\+\_\+\+Helpers\+::init(...) and \textbackslash{}c M\+P\+I\+\_\+\+Helpers see$<$ a href=\char`\"{}../../../linear\+\_\+solvers/html/index.\+html\char`\"{}$>$ the linear solver tutorial$<$/a $>$ for details n n c oomph lib s block preconditioning framework is fully parallelised and can be used in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} same way as in a serial code$<$ HR $>$ subsection self\+\_\+tests How to include \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} demo codes into \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} self tests The configure flag c with mpi self tests includes c oomph lib s \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} demo driver codes into \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} self tests executed when c make c check is run The self tests require \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} executable to be run on two \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} command \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} spawns a two processor \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} job on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} target machine must be specified as an argument to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} configure flag For \mbox{\hyperlink{general__mpi_8txt_a1a87cbf19b7098172f49ad631a0f249d}{example}}
\end{DoxyCompactItemize}


\doxysubsection{Function Documentation}
\mbox{\Hypertarget{general__mpi_8txt_a8b4b24e305d32964339177a53bbbf0d1}\label{general__mpi_8txt_a8b4b24e305d32964339177a53bbbf0d1}} 
\index{general\_mpi.txt@{general\_mpi.txt}!functions@{functions}}
\index{functions@{functions}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{functions()}{functions()}}
{\footnotesize\ttfamily mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a$>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a$>$ The only \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward a single call to c \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} majority of c oomph lib s multi physics helper functions (\begin{DoxyParamCaption}\item[{$<$ em $>$ e.\+g.$<$/em $>$ \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} automatic setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} fluid load on solid \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} in F\+SI \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}};\mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} determination of \char`\"{}source \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}}\char`\"{} in multi-\/field \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}};}]{etc }\end{DoxyParamCaption})}

\mbox{\Hypertarget{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}\label{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}} 
\index{general\_mpi.txt@{general\_mpi.txt}!the@{the}}
\index{the@{the}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{the()}{the()}}
{\footnotesize\ttfamily endcode n Running the code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on the specific problem In most the most computationally expensive tasks are the setup of the Jacobian matrix and the solution of the linear systems When a driver code is run on multiple each processor assembles contributions to the Jacobian matrix from a different subset of the dividing the work of the assembly between \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} In our the assembly of the Jacobian matrix tends to scale very well with the number of \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} The \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} performance of the (\begin{DoxyParamCaption}\item[{third-\/}]{party }\end{DoxyParamCaption})}



\doxysubsection{Variable Documentation}
\mbox{\Hypertarget{general__mpi_8txt_a27976d67be78d15a2ae29ab248ac1b68}\label{general__mpi_8txt_a27976d67be78d15a2ae29ab248ac1b68}} 
\index{general\_mpi.txt@{general\_mpi.txt}!applications@{applications}}
\index{applications@{applications}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{applications}{applications}}
{\footnotesize\ttfamily endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most applications}



Definition at line 122 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a9b70270f19cce3ae52cb4e5690ce39aa}\label{general__mpi_8txt_a9b70270f19cce3ae52cb4e5690ce39aa}} 
\index{general\_mpi.txt@{general\_mpi.txt}!basics@{basics}}
\index{basics@{basics}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{basics}{basics}}
{\footnotesize\ttfamily mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref basics}



Definition at line 5 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a2ddf49a0be288edadd9ec711f1fad73a}\label{general__mpi_8txt_a2ddf49a0be288edadd9ec711f1fad73a}} 
\index{general\_mpi.txt@{general\_mpi.txt}!CC@{CC}}
\index{CC@{CC}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{CC}{CC}}
{\footnotesize\ttfamily mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a$>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a$>$ The only \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward a single call to c \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} majority of c oomph lib s multi physics helper \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} user may have to intervene in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution process and or be aware of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} consequences of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} section ref domain\+\_\+decomposition provides an overview of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} underlying design used for problem distribution within c oomph lib A number of$<$a href=\char`\"{}../../../example\+\_\+code\+\_\+list/html/index.\+html\#\mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}}\char`\"{}$>$ demo driver codes for distributed \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}}$<$/a$>$ are provided and any additional issues are discussed in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} accompanying tutorials n n$<$HR$>$$<$HR$>$ section \mbox{\hyperlink{general__mpi_8txt_a9b70270f19cce3ae52cb4e5690ce39aa}{basics}} Basic \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} usage subsection installation How to build install oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} To compile c oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} you must specify \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} configure flag n n code enable M\+PI endcode n If you use c oomph lib s$<$code$>$ autogen sh$<$/code$>$ script to build \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library you should add this line to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}}$<$code$>$ config configure\+\_\+options current$<$/code$>$ file You should also ensure \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} appropriate \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} compilers are specified by \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} c c CC}



Definition at line 59 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a56da20c03154f15d6bd52e24a26d4d91}\label{general__mpi_8txt_a56da20c03154f15d6bd52e24a26d4d91}} 
\index{general\_mpi.txt@{general\_mpi.txt}!CXX@{CXX}}
\index{CXX@{CXX}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{CXX}{CXX}}
{\footnotesize\ttfamily mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a$>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a$>$ The only \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward a single call to c \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} majority of c oomph lib s multi physics helper \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} user may have to intervene in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution process and or be aware of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} consequences of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} section ref domain\+\_\+decomposition provides an overview of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} underlying design used for problem distribution within c oomph lib A number of$<$a href=\char`\"{}../../../example\+\_\+code\+\_\+list/html/index.\+html\#\mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}}\char`\"{}$>$ demo driver codes for distributed \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}}$<$/a$>$ are provided and any additional issues are discussed in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} accompanying tutorials n n$<$HR$>$$<$HR$>$ section \mbox{\hyperlink{general__mpi_8txt_a9b70270f19cce3ae52cb4e5690ce39aa}{basics}} Basic \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} usage subsection installation How to build install oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} To compile c oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} you must specify \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} configure flag n n code enable M\+PI endcode n If you use c oomph lib s$<$code$>$ autogen sh$<$/code$>$ script to build \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library you should add this line to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}}$<$code$>$ config configure\+\_\+options current$<$/code$>$ file You should also ensure \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} appropriate \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} compilers are specified by \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} c C\+XX}



Definition at line 59 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}\label{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}} 
\index{general\_mpi.txt@{general\_mpi.txt}!elements@{elements}}
\index{elements@{elements}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{elements}{elements}}
{\footnotesize\ttfamily endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive tasks are \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems When a driver code is run on multiple each processor assembles contributions to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix from a different subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} elements}



Definition at line 126 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a1a87cbf19b7098172f49ad631a0f249d}\label{general__mpi_8txt_a1a87cbf19b7098172f49ad631a0f249d}} 
\index{general\_mpi.txt@{general\_mpi.txt}!example@{example}}
\index{example@{example}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{example}{example}}
{\footnotesize\ttfamily endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive tasks are \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems When a driver code is run on multiple each processor assembles contributions to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix from a different subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} dividing \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} work of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly between \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} In our \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix tends to scale very well with \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} number of \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} The \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} performance of$<$em$>$ e g$<$/em$>$ \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} speed of your machine s etc n n The M\+PI \mbox{\hyperlink{general__mpi_8txt_a58dff0adb83d6245f6d35c3df6615412}{header}} file c mpi h is included in c oomph lib s generic so it is not necessary to include it in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} driver code The \mbox{\hyperlink{general__mpi_8txt_a8b4b24e305d32964339177a53bbbf0d1}{functions}} c M\+P\+I\+\_\+\+Helpers\+::init (...) and \textbackslash{}c M\+P\+I\+\_\+\+Helpers see$<$a href=\char`\"{}../../../linear\+\_\+solvers/html/index.\+html\char`\"{}$>$ the linear solver tutorial$<$/a$>$ for details n n c oomph lib s block preconditioning framework is fully parallelised and can be used in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} same way as in a serial code$<$HR$>$ subsection self\+\_\+tests How to include \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} demo codes into \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} self tests The configure flag c with mpi self tests includes c oomph lib s \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} demo driver codes into \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} self tests executed when c make c check is run The self tests require \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} executable to be run on two \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} command \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} spawns a two processor \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} job on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} target machine must be specified as an argument to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} configure flag For example}



Definition at line 138 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a07c5407165a6de8c6d14fa82165c75d8}\label{general__mpi_8txt_a07c5407165a6de8c6d14fa82165c75d8}} 
\index{general\_mpi.txt@{general\_mpi.txt}!experience@{experience}}
\index{experience@{experience}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{experience}{experience}}
{\footnotesize\ttfamily endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive tasks are \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems When a driver code is run on multiple each processor assembles contributions to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix from a different subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} dividing \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} work of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly between \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} In our experience}



Definition at line 127 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a58dff0adb83d6245f6d35c3df6615412}\label{general__mpi_8txt_a58dff0adb83d6245f6d35c3df6615412}} 
\index{general\_mpi.txt@{general\_mpi.txt}!header@{header}}
\index{header@{header}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{header}{header}}
{\footnotesize\ttfamily endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive tasks are \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems When a driver code is run on multiple each processor assembles contributions to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix from a different subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} dividing \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} work of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly between \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} In our \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix tends to scale very well with \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} number of \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} The \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} performance of$<$em$>$ e g$<$/em$>$ \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} speed of your machine s etc n n The M\+PI header file c mpi h is included in c oomph lib s generic header}



Definition at line 137 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a841aef979a2437ad61f7a57455985e99}\label{general__mpi_8txt_a841aef979a2437ad61f7a57455985e99}} 
\index{general\_mpi.txt@{general\_mpi.txt}!Hence@{Hence}}
\index{Hence@{Hence}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{Hence}{Hence}}
{\footnotesize\ttfamily mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a$>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a$>$ The only \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward a single call to c \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} majority of c oomph lib s multi physics helper \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} user may have to intervene in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution process and or be aware of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} consequences of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution Hence}



Definition at line 32 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a4547f3139c2429c2424a100dd9e464e8}\label{general__mpi_8txt_a4547f3139c2429c2424a100dd9e464e8}} 
\index{general\_mpi.txt@{general\_mpi.txt}!instance@{instance}}
\index{instance@{instance}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{instance}{instance}}
{\footnotesize\ttfamily mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a$>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a$>$ The only \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward a single call to c \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} majority of c oomph lib s multi physics helper \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} user may have to intervene in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution process and or be aware of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} consequences of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} section ref domain\+\_\+decomposition provides an overview of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} underlying design used for problem distribution within c oomph lib A number of$<$a href=\char`\"{}../../../example\+\_\+code\+\_\+list/html/index.\+html\#\mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}}\char`\"{}$>$ demo driver codes for distributed \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}}$<$/a$>$ are provided and any additional issues are discussed in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} accompanying tutorials n n$<$HR$>$$<$HR$>$ section \mbox{\hyperlink{general__mpi_8txt_a9b70270f19cce3ae52cb4e5690ce39aa}{basics}} Basic \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} usage subsection installation How to build install oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} To compile c oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} you must specify \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} configure flag n n code enable M\+PI endcode n If you use c oomph lib s$<$code$>$ autogen sh$<$/code$>$ script to build \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library you should add this line to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}}$<$code$>$ config configure\+\_\+options current$<$/code$>$ file You should also ensure \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} appropriate \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} compilers are specified by \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} c c c F77 and c LD flags For instance}



Definition at line 59 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_ae749d01bee77d0ded7e8930a7957cd4b}\label{general__mpi_8txt_ae749d01bee77d0ded7e8930a7957cd4b}} 
\index{general\_mpi.txt@{general\_mpi.txt}!interconnects@{interconnects}}
\index{interconnects@{interconnects}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{interconnects}{interconnects}}
{\footnotesize\ttfamily endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive tasks are \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems When a driver code is run on multiple each processor assembles contributions to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix from a different subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} dividing \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} work of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly between \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} In our \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix tends to scale very well with \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} number of \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} The \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} performance of$<$em$>$ e g$<$/em$>$ \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} speed of your machine s interconnects}



Definition at line 134 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a7899efb3fdcf2096a8ff9a2c6882c752}\label{general__mpi_8txt_a7899efb3fdcf2096a8ff9a2c6882c752}} 
\index{general\_mpi.txt@{general\_mpi.txt}!iteration@{iteration}}
\index{iteration@{iteration}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{iteration}{iteration}}
{\footnotesize\ttfamily mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton iteration}



Definition at line 12 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}\label{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}} 
\index{general\_mpi.txt@{general\_mpi.txt}!parallel@{parallel}}
\index{parallel@{parallel}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{parallel}{parallel}}
{\footnotesize\ttfamily mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in parallel Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a$>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a$>$ The only parallel task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward a single call to c \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} majority of c oomph lib s multi physics helper \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} user may have to intervene in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution process and or be aware of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} consequences of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} section ref domain\+\_\+decomposition provides an overview of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} underlying design used for problem distribution within c oomph lib A number of$<$a href=\char`\"{}../../../example\+\_\+code\+\_\+list/html/index.\+html\#parallel\char`\"{}$>$ demo driver codes for distributed \mbox{\hyperlink{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}{problems}}$<$/a$>$ are provided and any additional issues are discussed in \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} accompanying tutorials n n$<$HR$>$$<$HR$>$ section \mbox{\hyperlink{general__mpi_8txt_a9b70270f19cce3ae52cb4e5690ce39aa}{basics}} Basic parallel usage subsection installation How to build install oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} To compile c oomph lib with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} you must specify \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} configure flag n n code enable M\+PI endcode n If you use c oomph lib s$<$code$>$ autogen sh$<$/code$>$ script to build \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library you should add this line to \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}}$<$code$>$ config configure\+\_\+options current$<$/code$>$ file You should also ensure \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} appropriate parallel compilers are specified by \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} c c c F77 and c LD flags For if you use$<$a href=\char`\"{}http\+: you should use \textbackslash{}c \mbox{\hyperlink{general__mpi_8txt_a56da20c03154f15d6bd52e24a26d4d91}{C\+XX}}=mpic++, \textbackslash{}c \mbox{\hyperlink{general__mpi_8txt_a2ddf49a0be288edadd9ec711f1fad73a}{CC}}=mpicc, \textbackslash{}c F77=mpif77 and \textbackslash{}c LD=mpif77. \textbackslash{}n\textbackslash{}n-\/ When \textbackslash{}c oomph-\/lib is built with M\+PI \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}}, \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} macro \textbackslash{}c O\+O\+M\+P\+H\+\_\+\+H\+A\+S\+\_\+\+M\+PI is defined. It is used to isolate parallel sections of code to ensure \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library can be used in serial and parallel\+: $<$em$>$ e.\+g. $<$/em$>$ \textbackslash{}n\textbackslash{}n \textbackslash{}code \mbox{[}...\mbox{]} \#ifdef O\+O\+M\+P\+H\+\_\+\+H\+A\+S\+\_\+\+M\+PI std\+::cout $<$$<$ \char`\"{}This code has been compiled with mpi \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}} \textbackslash{}n \char`\"{} $<$$<$ \char`\"{}and is running on \char`\"{} $<$$<$ Communicator\+\_\+pt-\/$>$ nproc () $<$$<$ \char`\"{} processors. \char`\"{} $<$$<$ std std\+::cout$<$$<$ \char`\"{}This code has been compiled without mpi \mbox{\hyperlink{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}{support}}\char`\"{} $<$$<$ std\+::endl; \#endif \mbox{[}...\mbox{]} \textbackslash{}endcode \textbackslash{}n.\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#$<$HR$>$ subsection running How to run a driver code in parallel M\+PI$<$strong$>$ must$<$/strong$>$ be initialised in every driver code \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} is to be run in parallel}



Definition at line 75 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}\label{general__mpi_8txt_a8e5f8f7890b2068b889fad478cd9bf2b}} 
\index{general\_mpi.txt@{general\_mpi.txt}!problems@{problems}}
\index{problems@{problems}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{problems}{problems}}
{\footnotesize\ttfamily mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI discussed below in ref many of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive phases of a typical computation are automatically performed in \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} Examples of automatically parallelised tasks include The assembly of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} residual vector in Newton s method Error estimation The solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems within \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Newton and any preconditioning operations performed within c oomph lib s$<$a href=\char`\"{}../../../mpi/block\+\_\+preconditioners/html/index.\+html\char`\"{}$>$ block preconditioning framework$<$/a$>$ which relies heavily on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library s$<$a href=\char`\"{}../../../mpi/distributed\+\_\+linear\+\_\+algebra\+\_\+infrastructure/html/index.\+html\char`\"{}$>$ distributed linear algebra infrastructure$<$/a$>$ The only \mbox{\hyperlink{general__mpi_8txt_aa0367dea3ef7bca821f17c6d18c00b44}{parallel}} task \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} requires user intervention is \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} distribution of a problem over multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} so \mbox{\hyperlink{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}{that}} each processor stores a subset of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} \mbox{\hyperlink{general__mpi_8txt_a4fea119ad352a5e9af4b03c3e6fbb6da}{elements}} For straightforward problems}



Definition at line 24 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}\label{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}} 
\index{general\_mpi.txt@{general\_mpi.txt}!processors@{processors}}
\index{processors@{processors}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{processors}{processors}}
{\footnotesize\ttfamily endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple processors should immediately lead to a although this depends on \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} specific problem In most \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} most computationally expensive tasks are \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} setup of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} Jacobian matrix and \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} solution of \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} linear systems When a driver code is run on multiple processors}



Definition at line 124 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_af907a2a24a4ba5cbc2a07b1b01229f35}\label{general__mpi_8txt_af907a2a24a4ba5cbc2a07b1b01229f35}} 
\index{general\_mpi.txt@{general\_mpi.txt}!speedup@{speedup}}
\index{speedup@{speedup}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{speedup}{speedup}}
{\footnotesize\ttfamily endcode n Running \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} code on multiple \mbox{\hyperlink{general__mpi_8txt_a26148a209e819999805bbbeaec42960d}{processors}} should immediately lead to a speedup}



Definition at line 121 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}\label{general__mpi_8txt_a32318aade1c612d244cd336e0ceea5b7}} 
\index{general\_mpi.txt@{general\_mpi.txt}!support@{support}}
\index{support@{support}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{support}{support}}
{\footnotesize\ttfamily mainpage Parallel processing c oomph lib is designed so provided \mbox{\hyperlink{general__mpi_8txt_a20237347d4fd9890f78a86a6c791c889}{the}} library is compiled with M\+PI support}



Definition at line 4 of file general\+\_\+mpi.\+txt.

\mbox{\Hypertarget{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}\label{general__mpi_8txt_a5c60c1f0bdfccd2ab5faedc8135320aa}} 
\index{general\_mpi.txt@{general\_mpi.txt}!that@{that}}
\index{that@{that}!general\_mpi.txt@{general\_mpi.txt}}
\doxysubsubsection{\texorpdfstring{that}{that}}
{\footnotesize\ttfamily mainpage Parallel processing c oomph lib is designed so that}



Definition at line 3 of file general\+\_\+mpi.\+txt.

